{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "train_mnist = loadmat('mnist_train.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = train_mnist['train_X']\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = train_mnist['train_labels']\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### initialize pytorch dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MyMNISTDataset(object):\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "    \n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "dataset = MyMNISTDataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=2000, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE model and training configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 250 \n",
    "epochs = 200 \n",
    "rnd_seed = 5\n",
    "log_interval = 10\n",
    "\n",
    "input_dim, h1_dim, h2_dim, h3_dim, embed_dim = 784, 500, 500, 2000, 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define VAE model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        # encoder phase\n",
    "        self.fc1 = nn.Linear(input_dim, h1_dim)\n",
    "        self.fc2 = nn.Linear(h1_dim, h2_dim)\n",
    "        self.fc3 = nn.Linear(h2_dim, h3_dim)\n",
    "        self.fc41 = nn.Linear(h3_dim, embed_dim)\n",
    "        self.fc42 = nn.Linear(h3_dim, embed_dim)\n",
    "        # decoder phase\n",
    "        self.fc5 = nn.Linear(embed_dim, h3_dim)\n",
    "        self.fc6 = nn.Linear(h3_dim, h2_dim)\n",
    "        self.fc7 = nn.Linear(h2_dim, h1_dim)\n",
    "        self.fc8 = nn.Linear(h1_dim, input_dim)\n",
    "        # define activation\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def encode(self, x):\n",
    "        h3 = self.relu(self.fc3(self.relu(self.fc2(self.relu(self.fc1(x))))))\n",
    "        return self.fc41(h3), self.fc42(h3)\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        eps = torch.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(eps)\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    def decode(self, z):\n",
    "        recon = self.sigmoid(self.fc8(self.relu(self.fc7(self.relu(self.fc6(self.relu(self.fc5(z))))))))\n",
    "        return recon\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define ELOB loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reconstruction_function = nn.BCELoss()\n",
    "reconstruction_function.size_average = False\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = reconstruction_function(recon_x, x)\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n",
    "    KLD = torch.sum(KLD_element).mul_(-0.5)\n",
    "\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = VAE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "dtype = torch.FloatTensor\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(dataloader):\n",
    "        data = Variable(data.float())\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.data[0]\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(dataloader.dataset),\n",
    "                100. * batch_idx / len(dataloader),\n",
    "                loss.data[0] / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(dataloader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finnally !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 256.880313\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 229.321687\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 216.801969\n",
      "====> Epoch: 1 Average loss: 217.0720\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 278.724125\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 216.016063\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 213.765984\n",
      "====> Epoch: 2 Average loss: 209.4080\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 257.375891\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 216.351578\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 212.943531\n",
      "====> Epoch: 3 Average loss: 206.0136\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 257.835281\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 217.230031\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 211.614734\n",
      "====> Epoch: 4 Average loss: 205.8772\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 257.045391\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 215.438312\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 209.859000\n",
      "====> Epoch: 5 Average loss: 203.9322\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 247.800297\n",
      "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 215.364391\n",
      "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 209.897297\n",
      "====> Epoch: 6 Average loss: 204.2334\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 244.967766\n",
      "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 219.281937\n",
      "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 208.107016\n",
      "====> Epoch: 7 Average loss: 201.0661\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 246.421031\n",
      "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 214.475188\n",
      "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 208.115672\n",
      "====> Epoch: 8 Average loss: 199.8459\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 238.954375\n",
      "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 213.995469\n",
      "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 204.943750\n",
      "====> Epoch: 9 Average loss: 197.6663\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 236.481391\n",
      "Train Epoch: 10 [20000/60000 (33%)]\tLoss: 212.596531\n",
      "Train Epoch: 10 [40000/60000 (67%)]\tLoss: 202.747406\n",
      "====> Epoch: 10 Average loss: 195.9862\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 234.806891\n",
      "Train Epoch: 11 [20000/60000 (33%)]\tLoss: 213.669250\n",
      "Train Epoch: 11 [40000/60000 (67%)]\tLoss: 204.152156\n",
      "====> Epoch: 11 Average loss: 195.3710\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 230.628453\n",
      "Train Epoch: 12 [20000/60000 (33%)]\tLoss: 211.564953\n",
      "Train Epoch: 12 [40000/60000 (67%)]\tLoss: 203.627078\n",
      "====> Epoch: 12 Average loss: 193.0855\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 228.203891\n",
      "Train Epoch: 13 [20000/60000 (33%)]\tLoss: 213.425563\n",
      "Train Epoch: 13 [40000/60000 (67%)]\tLoss: 204.228594\n",
      "====> Epoch: 13 Average loss: 193.1526\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 228.388375\n",
      "Train Epoch: 14 [20000/60000 (33%)]\tLoss: 213.061187\n",
      "Train Epoch: 14 [40000/60000 (67%)]\tLoss: 203.647344\n",
      "====> Epoch: 14 Average loss: 193.3004\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 224.981219\n",
      "Train Epoch: 15 [20000/60000 (33%)]\tLoss: 213.526625\n",
      "Train Epoch: 15 [40000/60000 (67%)]\tLoss: 204.963422\n",
      "====> Epoch: 15 Average loss: 193.0654\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 236.836156\n",
      "Train Epoch: 16 [20000/60000 (33%)]\tLoss: 220.404656\n",
      "Train Epoch: 16 [40000/60000 (67%)]\tLoss: 206.000094\n",
      "====> Epoch: 16 Average loss: 194.9075\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 232.830453\n",
      "Train Epoch: 17 [20000/60000 (33%)]\tLoss: 224.880188\n",
      "Train Epoch: 17 [40000/60000 (67%)]\tLoss: 204.616469\n",
      "====> Epoch: 17 Average loss: 195.0600\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 228.823453\n",
      "Train Epoch: 18 [20000/60000 (33%)]\tLoss: 213.971172\n",
      "Train Epoch: 18 [40000/60000 (67%)]\tLoss: 203.685234\n",
      "====> Epoch: 18 Average loss: 191.7683\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 223.856422\n",
      "Train Epoch: 19 [20000/60000 (33%)]\tLoss: 212.366187\n",
      "Train Epoch: 19 [40000/60000 (67%)]\tLoss: 203.084891\n",
      "====> Epoch: 19 Average loss: 191.3572\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 218.736594\n",
      "Train Epoch: 20 [20000/60000 (33%)]\tLoss: 212.915875\n",
      "Train Epoch: 20 [40000/60000 (67%)]\tLoss: 202.415609\n",
      "====> Epoch: 20 Average loss: 190.2972\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 213.810953\n",
      "Train Epoch: 21 [20000/60000 (33%)]\tLoss: 212.744297\n",
      "Train Epoch: 21 [40000/60000 (67%)]\tLoss: 203.966656\n",
      "====> Epoch: 21 Average loss: 192.9781\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 210.159063\n",
      "Train Epoch: 22 [20000/60000 (33%)]\tLoss: 215.261219\n",
      "Train Epoch: 22 [40000/60000 (67%)]\tLoss: 202.238641\n",
      "====> Epoch: 22 Average loss: 191.2348\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 240.801375\n",
      "Train Epoch: 23 [20000/60000 (33%)]\tLoss: 214.743406\n",
      "Train Epoch: 23 [40000/60000 (67%)]\tLoss: 204.382609\n",
      "====> Epoch: 23 Average loss: 195.2651\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 223.001469\n",
      "Train Epoch: 24 [20000/60000 (33%)]\tLoss: 214.962578\n",
      "Train Epoch: 24 [40000/60000 (67%)]\tLoss: 202.664484\n",
      "====> Epoch: 24 Average loss: 193.0751\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 209.212547\n",
      "Train Epoch: 25 [20000/60000 (33%)]\tLoss: 214.264219\n",
      "Train Epoch: 25 [40000/60000 (67%)]\tLoss: 202.789375\n",
      "====> Epoch: 25 Average loss: 192.0472\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 206.966141\n",
      "Train Epoch: 26 [20000/60000 (33%)]\tLoss: 211.729453\n",
      "Train Epoch: 26 [40000/60000 (67%)]\tLoss: 201.581359\n",
      "====> Epoch: 26 Average loss: 188.5628\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 208.171641\n",
      "Train Epoch: 27 [20000/60000 (33%)]\tLoss: 211.319469\n",
      "Train Epoch: 27 [40000/60000 (67%)]\tLoss: 201.066422\n",
      "====> Epoch: 27 Average loss: 188.6061\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 207.685062\n",
      "Train Epoch: 28 [20000/60000 (33%)]\tLoss: 214.459391\n",
      "Train Epoch: 28 [40000/60000 (67%)]\tLoss: 200.496344\n",
      "====> Epoch: 28 Average loss: 189.3107\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 210.733172\n",
      "Train Epoch: 29 [20000/60000 (33%)]\tLoss: 215.728781\n",
      "Train Epoch: 29 [40000/60000 (67%)]\tLoss: 199.282703\n",
      "====> Epoch: 29 Average loss: 190.3475\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 207.619391\n",
      "Train Epoch: 30 [20000/60000 (33%)]\tLoss: 213.452937\n",
      "Train Epoch: 30 [40000/60000 (67%)]\tLoss: 200.228828\n",
      "====> Epoch: 30 Average loss: 188.8146\n",
      "Train Epoch: 31 [0/60000 (0%)]\tLoss: 202.878609\n",
      "Train Epoch: 31 [20000/60000 (33%)]\tLoss: 211.187641\n",
      "Train Epoch: 31 [40000/60000 (67%)]\tLoss: 198.703281\n",
      "====> Epoch: 31 Average loss: 187.5144\n",
      "Train Epoch: 32 [0/60000 (0%)]\tLoss: 208.450047\n",
      "Train Epoch: 32 [20000/60000 (33%)]\tLoss: 211.689672\n",
      "Train Epoch: 32 [40000/60000 (67%)]\tLoss: 198.913547\n",
      "====> Epoch: 32 Average loss: 188.0236\n",
      "Train Epoch: 33 [0/60000 (0%)]\tLoss: 226.541516\n",
      "Train Epoch: 33 [20000/60000 (33%)]\tLoss: 212.492234\n",
      "Train Epoch: 33 [40000/60000 (67%)]\tLoss: 203.582547\n",
      "====> Epoch: 33 Average loss: 196.1828\n",
      "Train Epoch: 34 [0/60000 (0%)]\tLoss: 220.263937\n",
      "Train Epoch: 34 [20000/60000 (33%)]\tLoss: 215.340500\n",
      "Train Epoch: 34 [40000/60000 (67%)]\tLoss: 200.257031\n",
      "====> Epoch: 34 Average loss: 196.0784\n",
      "Train Epoch: 35 [0/60000 (0%)]\tLoss: 214.887203\n",
      "Train Epoch: 35 [20000/60000 (33%)]\tLoss: 211.961484\n",
      "Train Epoch: 35 [40000/60000 (67%)]\tLoss: 202.332703\n",
      "====> Epoch: 35 Average loss: 195.3033\n",
      "Train Epoch: 36 [0/60000 (0%)]\tLoss: 235.193125\n",
      "Train Epoch: 36 [20000/60000 (33%)]\tLoss: 212.983125\n",
      "Train Epoch: 36 [40000/60000 (67%)]\tLoss: 202.280969\n",
      "====> Epoch: 36 Average loss: 190.2709\n",
      "Train Epoch: 37 [0/60000 (0%)]\tLoss: 206.900406\n",
      "Train Epoch: 37 [20000/60000 (33%)]\tLoss: 211.671219\n",
      "Train Epoch: 37 [40000/60000 (67%)]\tLoss: 199.834719\n",
      "====> Epoch: 37 Average loss: 187.7605\n",
      "Train Epoch: 38 [0/60000 (0%)]\tLoss: 215.723797\n",
      "Train Epoch: 38 [20000/60000 (33%)]\tLoss: 211.608734\n",
      "Train Epoch: 38 [40000/60000 (67%)]\tLoss: 201.360328\n",
      "====> Epoch: 38 Average loss: 188.3735\n",
      "Train Epoch: 39 [0/60000 (0%)]\tLoss: 206.531531\n",
      "Train Epoch: 39 [20000/60000 (33%)]\tLoss: 211.161875\n",
      "Train Epoch: 39 [40000/60000 (67%)]\tLoss: 200.314141\n",
      "====> Epoch: 39 Average loss: 187.5363\n",
      "Train Epoch: 40 [0/60000 (0%)]\tLoss: 203.276109\n",
      "Train Epoch: 40 [20000/60000 (33%)]\tLoss: 209.927406\n",
      "Train Epoch: 40 [40000/60000 (67%)]\tLoss: 200.295516\n",
      "====> Epoch: 40 Average loss: 185.4701\n",
      "Train Epoch: 41 [0/60000 (0%)]\tLoss: 206.356203\n",
      "Train Epoch: 41 [20000/60000 (33%)]\tLoss: 211.426797\n",
      "Train Epoch: 41 [40000/60000 (67%)]\tLoss: 201.447875\n",
      "====> Epoch: 41 Average loss: 188.0134\n",
      "Train Epoch: 42 [0/60000 (0%)]\tLoss: 212.391266\n",
      "Train Epoch: 42 [20000/60000 (33%)]\tLoss: 213.532734\n",
      "Train Epoch: 42 [40000/60000 (67%)]\tLoss: 201.077047\n",
      "====> Epoch: 42 Average loss: 189.6702\n",
      "Train Epoch: 43 [0/60000 (0%)]\tLoss: 205.497469\n",
      "Train Epoch: 43 [20000/60000 (33%)]\tLoss: 209.332703\n",
      "Train Epoch: 43 [40000/60000 (67%)]\tLoss: 200.532187\n",
      "====> Epoch: 43 Average loss: 190.0667\n",
      "Train Epoch: 44 [0/60000 (0%)]\tLoss: 209.667078\n",
      "Train Epoch: 44 [20000/60000 (33%)]\tLoss: 209.974844\n",
      "Train Epoch: 44 [40000/60000 (67%)]\tLoss: 200.889625\n",
      "====> Epoch: 44 Average loss: 187.3686\n",
      "Train Epoch: 45 [0/60000 (0%)]\tLoss: 217.619094\n",
      "Train Epoch: 45 [20000/60000 (33%)]\tLoss: 209.241187\n",
      "Train Epoch: 45 [40000/60000 (67%)]\tLoss: 199.942844\n",
      "====> Epoch: 45 Average loss: 193.3953\n",
      "Train Epoch: 46 [0/60000 (0%)]\tLoss: 230.629141\n",
      "Train Epoch: 46 [20000/60000 (33%)]\tLoss: 212.480250\n",
      "Train Epoch: 46 [40000/60000 (67%)]\tLoss: 201.046969\n",
      "====> Epoch: 46 Average loss: 191.5530\n",
      "Train Epoch: 47 [0/60000 (0%)]\tLoss: 208.985187\n",
      "Train Epoch: 47 [20000/60000 (33%)]\tLoss: 211.131297\n",
      "Train Epoch: 47 [40000/60000 (67%)]\tLoss: 199.237250\n",
      "====> Epoch: 47 Average loss: 188.6584\n",
      "Train Epoch: 48 [0/60000 (0%)]\tLoss: 203.461141\n",
      "Train Epoch: 48 [20000/60000 (33%)]\tLoss: 208.622688\n",
      "Train Epoch: 48 [40000/60000 (67%)]\tLoss: 201.063344\n",
      "====> Epoch: 48 Average loss: 185.6361\n",
      "Train Epoch: 49 [0/60000 (0%)]\tLoss: 206.215594\n",
      "Train Epoch: 49 [20000/60000 (33%)]\tLoss: 208.623969\n",
      "Train Epoch: 49 [40000/60000 (67%)]\tLoss: 200.604594\n",
      "====> Epoch: 49 Average loss: 185.9661\n",
      "Train Epoch: 50 [0/60000 (0%)]\tLoss: 207.121469\n",
      "Train Epoch: 50 [20000/60000 (33%)]\tLoss: 207.554281\n",
      "Train Epoch: 50 [40000/60000 (67%)]\tLoss: 202.301187\n",
      "====> Epoch: 50 Average loss: 185.6841\n",
      "Train Epoch: 51 [0/60000 (0%)]\tLoss: 206.596156\n",
      "Train Epoch: 51 [20000/60000 (33%)]\tLoss: 208.095594\n",
      "Train Epoch: 51 [40000/60000 (67%)]\tLoss: 200.322422\n",
      "====> Epoch: 51 Average loss: 185.4536\n",
      "Train Epoch: 52 [0/60000 (0%)]\tLoss: 217.531391\n",
      "Train Epoch: 52 [20000/60000 (33%)]\tLoss: 207.408406\n",
      "Train Epoch: 52 [40000/60000 (67%)]\tLoss: 200.696562\n",
      "====> Epoch: 52 Average loss: 187.6231\n",
      "Train Epoch: 53 [0/60000 (0%)]\tLoss: 203.368797\n",
      "Train Epoch: 53 [20000/60000 (33%)]\tLoss: 207.951125\n",
      "Train Epoch: 53 [40000/60000 (67%)]\tLoss: 199.278609\n",
      "====> Epoch: 53 Average loss: 186.6731\n",
      "Train Epoch: 54 [0/60000 (0%)]\tLoss: 203.940844\n",
      "Train Epoch: 54 [20000/60000 (33%)]\tLoss: 207.733016\n",
      "Train Epoch: 54 [40000/60000 (67%)]\tLoss: 200.772781\n",
      "====> Epoch: 54 Average loss: 185.2042\n",
      "Train Epoch: 55 [0/60000 (0%)]\tLoss: 207.968094\n",
      "Train Epoch: 55 [20000/60000 (33%)]\tLoss: 206.206500\n",
      "Train Epoch: 55 [40000/60000 (67%)]\tLoss: 200.190297\n",
      "====> Epoch: 55 Average loss: 185.5053\n",
      "Train Epoch: 56 [0/60000 (0%)]\tLoss: 209.231672\n",
      "Train Epoch: 56 [20000/60000 (33%)]\tLoss: 208.480844\n",
      "Train Epoch: 56 [40000/60000 (67%)]\tLoss: 199.418594\n",
      "====> Epoch: 56 Average loss: 184.9725\n",
      "Train Epoch: 57 [0/60000 (0%)]\tLoss: 205.476422\n",
      "Train Epoch: 57 [20000/60000 (33%)]\tLoss: 207.750766\n",
      "Train Epoch: 57 [40000/60000 (67%)]\tLoss: 200.694516\n",
      "====> Epoch: 57 Average loss: 184.1821\n",
      "Train Epoch: 58 [0/60000 (0%)]\tLoss: 212.067031\n",
      "Train Epoch: 58 [20000/60000 (33%)]\tLoss: 205.945984\n",
      "Train Epoch: 58 [40000/60000 (67%)]\tLoss: 198.989016\n",
      "====> Epoch: 58 Average loss: 184.0908\n",
      "Train Epoch: 59 [0/60000 (0%)]\tLoss: 225.234563\n",
      "Train Epoch: 59 [20000/60000 (33%)]\tLoss: 206.341531\n",
      "Train Epoch: 59 [40000/60000 (67%)]\tLoss: 200.407875\n",
      "====> Epoch: 59 Average loss: 184.6342\n",
      "Train Epoch: 60 [0/60000 (0%)]\tLoss: 214.646016\n",
      "Train Epoch: 60 [20000/60000 (33%)]\tLoss: 204.126031\n",
      "Train Epoch: 60 [40000/60000 (67%)]\tLoss: 199.213203\n",
      "====> Epoch: 60 Average loss: 183.4692\n",
      "Train Epoch: 61 [0/60000 (0%)]\tLoss: 225.115203\n",
      "Train Epoch: 61 [20000/60000 (33%)]\tLoss: 202.158453\n",
      "Train Epoch: 61 [40000/60000 (67%)]\tLoss: 197.451516\n",
      "====> Epoch: 61 Average loss: 184.3604\n",
      "Train Epoch: 62 [0/60000 (0%)]\tLoss: 205.968391\n",
      "Train Epoch: 62 [20000/60000 (33%)]\tLoss: 205.880719\n",
      "Train Epoch: 62 [40000/60000 (67%)]\tLoss: 204.358875\n",
      "====> Epoch: 62 Average loss: 183.2569\n",
      "Train Epoch: 63 [0/60000 (0%)]\tLoss: 203.352016\n",
      "Train Epoch: 63 [20000/60000 (33%)]\tLoss: 202.006781\n",
      "Train Epoch: 63 [40000/60000 (67%)]\tLoss: 199.571156\n",
      "====> Epoch: 63 Average loss: 181.8856\n",
      "Train Epoch: 64 [0/60000 (0%)]\tLoss: 206.716578\n",
      "Train Epoch: 64 [20000/60000 (33%)]\tLoss: 204.403406\n",
      "Train Epoch: 64 [40000/60000 (67%)]\tLoss: 200.677328\n",
      "====> Epoch: 64 Average loss: 182.1363\n",
      "Train Epoch: 65 [0/60000 (0%)]\tLoss: 210.250250\n",
      "Train Epoch: 65 [20000/60000 (33%)]\tLoss: 202.286031\n",
      "Train Epoch: 65 [40000/60000 (67%)]\tLoss: 200.544563\n",
      "====> Epoch: 65 Average loss: 182.2317\n",
      "Train Epoch: 66 [0/60000 (0%)]\tLoss: 235.474391\n",
      "Train Epoch: 66 [20000/60000 (33%)]\tLoss: 201.859734\n",
      "Train Epoch: 66 [40000/60000 (67%)]\tLoss: 199.537453\n",
      "====> Epoch: 66 Average loss: 183.4243\n",
      "Train Epoch: 67 [0/60000 (0%)]\tLoss: 207.195063\n",
      "Train Epoch: 67 [20000/60000 (33%)]\tLoss: 201.928313\n",
      "Train Epoch: 67 [40000/60000 (67%)]\tLoss: 198.846672\n",
      "====> Epoch: 67 Average loss: 180.9516\n",
      "Train Epoch: 68 [0/60000 (0%)]\tLoss: 207.963422\n",
      "Train Epoch: 68 [20000/60000 (33%)]\tLoss: 200.727016\n",
      "Train Epoch: 68 [40000/60000 (67%)]\tLoss: 197.526266\n",
      "====> Epoch: 68 Average loss: 185.9724\n",
      "Train Epoch: 69 [0/60000 (0%)]\tLoss: 206.491297\n",
      "Train Epoch: 69 [20000/60000 (33%)]\tLoss: 199.353469\n",
      "Train Epoch: 69 [40000/60000 (67%)]\tLoss: 194.969000\n",
      "====> Epoch: 69 Average loss: 180.0741\n",
      "Train Epoch: 70 [0/60000 (0%)]\tLoss: 221.515953\n",
      "Train Epoch: 70 [20000/60000 (33%)]\tLoss: 204.214703\n",
      "Train Epoch: 70 [40000/60000 (67%)]\tLoss: 203.749313\n",
      "====> Epoch: 70 Average loss: 182.2978\n",
      "Train Epoch: 71 [0/60000 (0%)]\tLoss: 203.479312\n",
      "Train Epoch: 71 [20000/60000 (33%)]\tLoss: 199.205906\n",
      "Train Epoch: 71 [40000/60000 (67%)]\tLoss: 197.432406\n",
      "====> Epoch: 71 Average loss: 179.9962\n",
      "Train Epoch: 72 [0/60000 (0%)]\tLoss: 205.255625\n",
      "Train Epoch: 72 [20000/60000 (33%)]\tLoss: 199.255719\n",
      "Train Epoch: 72 [40000/60000 (67%)]\tLoss: 197.586844\n",
      "====> Epoch: 72 Average loss: 180.4464\n",
      "Train Epoch: 73 [0/60000 (0%)]\tLoss: 206.058203\n",
      "Train Epoch: 73 [20000/60000 (33%)]\tLoss: 201.726609\n",
      "Train Epoch: 73 [40000/60000 (67%)]\tLoss: 194.801922\n",
      "====> Epoch: 73 Average loss: 177.8565\n",
      "Train Epoch: 74 [0/60000 (0%)]\tLoss: 210.910328\n",
      "Train Epoch: 74 [20000/60000 (33%)]\tLoss: 198.563859\n",
      "Train Epoch: 74 [40000/60000 (67%)]\tLoss: 192.970359\n",
      "====> Epoch: 74 Average loss: 177.0364\n",
      "Train Epoch: 75 [0/60000 (0%)]\tLoss: 198.083844\n",
      "Train Epoch: 75 [20000/60000 (33%)]\tLoss: 195.673578\n",
      "Train Epoch: 75 [40000/60000 (67%)]\tLoss: 191.729344\n",
      "====> Epoch: 75 Average loss: 174.5660\n",
      "Train Epoch: 76 [0/60000 (0%)]\tLoss: 195.377328\n",
      "Train Epoch: 76 [20000/60000 (33%)]\tLoss: 197.308375\n",
      "Train Epoch: 76 [40000/60000 (67%)]\tLoss: 190.750234\n",
      "====> Epoch: 76 Average loss: 174.7116\n",
      "Train Epoch: 77 [0/60000 (0%)]\tLoss: 194.818484\n",
      "Train Epoch: 77 [20000/60000 (33%)]\tLoss: 195.501984\n",
      "Train Epoch: 77 [40000/60000 (67%)]\tLoss: 190.374281\n",
      "====> Epoch: 77 Average loss: 174.7743\n",
      "Train Epoch: 78 [0/60000 (0%)]\tLoss: 192.245844\n",
      "Train Epoch: 78 [20000/60000 (33%)]\tLoss: 202.733656\n",
      "Train Epoch: 78 [40000/60000 (67%)]\tLoss: 191.899438\n",
      "====> Epoch: 78 Average loss: 179.6575\n",
      "Train Epoch: 79 [0/60000 (0%)]\tLoss: 210.230031\n",
      "Train Epoch: 79 [20000/60000 (33%)]\tLoss: 199.091156\n",
      "Train Epoch: 79 [40000/60000 (67%)]\tLoss: 194.970828\n",
      "====> Epoch: 79 Average loss: 178.3126\n",
      "Train Epoch: 80 [0/60000 (0%)]\tLoss: 203.925828\n",
      "Train Epoch: 80 [20000/60000 (33%)]\tLoss: 200.588484\n",
      "Train Epoch: 80 [40000/60000 (67%)]\tLoss: 195.213578\n",
      "====> Epoch: 80 Average loss: 180.1023\n",
      "Train Epoch: 81 [0/60000 (0%)]\tLoss: 214.406125\n",
      "Train Epoch: 81 [20000/60000 (33%)]\tLoss: 193.745688\n",
      "Train Epoch: 81 [40000/60000 (67%)]\tLoss: 189.098516\n",
      "====> Epoch: 81 Average loss: 175.9168\n",
      "Train Epoch: 82 [0/60000 (0%)]\tLoss: 214.105469\n",
      "Train Epoch: 82 [20000/60000 (33%)]\tLoss: 194.713203\n",
      "Train Epoch: 82 [40000/60000 (67%)]\tLoss: 193.543563\n",
      "====> Epoch: 82 Average loss: 174.8845\n",
      "Train Epoch: 83 [0/60000 (0%)]\tLoss: 214.263250\n",
      "Train Epoch: 83 [20000/60000 (33%)]\tLoss: 198.410859\n",
      "Train Epoch: 83 [40000/60000 (67%)]\tLoss: 195.480328\n",
      "====> Epoch: 83 Average loss: 176.5817\n",
      "Train Epoch: 84 [0/60000 (0%)]\tLoss: 203.446656\n",
      "Train Epoch: 84 [20000/60000 (33%)]\tLoss: 194.656891\n",
      "Train Epoch: 84 [40000/60000 (67%)]\tLoss: 191.204828\n",
      "====> Epoch: 84 Average loss: 176.2684\n",
      "Train Epoch: 85 [0/60000 (0%)]\tLoss: 206.984906\n",
      "Train Epoch: 85 [20000/60000 (33%)]\tLoss: 195.231219\n",
      "Train Epoch: 85 [40000/60000 (67%)]\tLoss: 192.811750\n",
      "====> Epoch: 85 Average loss: 175.3969\n",
      "Train Epoch: 86 [0/60000 (0%)]\tLoss: 193.220094\n",
      "Train Epoch: 86 [20000/60000 (33%)]\tLoss: 199.835578\n",
      "Train Epoch: 86 [40000/60000 (67%)]\tLoss: 190.141875\n",
      "====> Epoch: 86 Average loss: 175.9421\n",
      "Train Epoch: 87 [0/60000 (0%)]\tLoss: 202.678063\n",
      "Train Epoch: 87 [20000/60000 (33%)]\tLoss: 195.399594\n",
      "Train Epoch: 87 [40000/60000 (67%)]\tLoss: 192.712141\n",
      "====> Epoch: 87 Average loss: 176.3895\n",
      "Train Epoch: 88 [0/60000 (0%)]\tLoss: 197.444922\n",
      "Train Epoch: 88 [20000/60000 (33%)]\tLoss: 194.827766\n",
      "Train Epoch: 88 [40000/60000 (67%)]\tLoss: 188.960266\n",
      "====> Epoch: 88 Average loss: 172.6115\n",
      "Train Epoch: 89 [0/60000 (0%)]\tLoss: 218.024750\n",
      "Train Epoch: 89 [20000/60000 (33%)]\tLoss: 194.203719\n",
      "Train Epoch: 89 [40000/60000 (67%)]\tLoss: 190.494328\n",
      "====> Epoch: 89 Average loss: 173.6607\n",
      "Train Epoch: 90 [0/60000 (0%)]\tLoss: 193.153516\n",
      "Train Epoch: 90 [20000/60000 (33%)]\tLoss: 197.797766\n",
      "Train Epoch: 90 [40000/60000 (67%)]\tLoss: 186.777500\n",
      "====> Epoch: 90 Average loss: 173.5107\n",
      "Train Epoch: 91 [0/60000 (0%)]\tLoss: 193.628750\n",
      "Train Epoch: 91 [20000/60000 (33%)]\tLoss: 191.835813\n",
      "Train Epoch: 91 [40000/60000 (67%)]\tLoss: 185.901453\n",
      "====> Epoch: 91 Average loss: 175.7367\n",
      "Train Epoch: 92 [0/60000 (0%)]\tLoss: 187.991219\n",
      "Train Epoch: 92 [20000/60000 (33%)]\tLoss: 192.936813\n",
      "Train Epoch: 92 [40000/60000 (67%)]\tLoss: 184.542563\n",
      "====> Epoch: 92 Average loss: 172.6834\n",
      "Train Epoch: 93 [0/60000 (0%)]\tLoss: 200.203828\n",
      "Train Epoch: 93 [20000/60000 (33%)]\tLoss: 197.506484\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
