{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "train_mnist = loadmat('mnist_train.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = train_mnist['train_X']\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 1)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = train_mnist['train_labels']\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "true_labels = labels.flatten().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### initialize pytorch dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MyMNISTDataset(object):\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "    \n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "dataset = MyMNISTDataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=200, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model & training configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 10 \n",
    "rnd_seed = 5\n",
    "log_interval = 10\n",
    "\n",
    "\n",
    "input_dim, y_dim = 784, 10\n",
    "encode_h1_dim = input_dim + y_dim\n",
    "h1_dim, h2_dim, h3_dim, embed_dim, output_dim  = 500, 500, 2000, 10, 784\n",
    "qy_h1_dim, qy_h2_dim = 500, 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define GmVAE model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GmVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GmVAE, self).__init__()\n",
    "        # encoder phase\n",
    "        self.fc01 = nn.Linear(input_dim, qy_h1_dim)\n",
    "        self.fc02 = nn.Linear(qy_h1_dim, qy_h2_dim)\n",
    "        self.fc03 = nn.Linear(qy_h2_dim, y_dim)\n",
    "        self.fc1 = nn.Linear(encode_h1_dim, h1_dim)\n",
    "        self.fc2 = nn.Linear(h1_dim, h2_dim)\n",
    "        self.fc3 = nn.Linear(h2_dim, h3_dim)\n",
    "        self.fc41 = nn.Linear(h3_dim, embed_dim)\n",
    "        self.fc42 = nn.Linear(h3_dim, embed_dim)\n",
    "        # decoder phase\n",
    "        self.fc13 = nn.Linear(y_dim, embed_dim)\n",
    "        self.fc14 = nn.Linear(y_dim, embed_dim)\n",
    "        self.fc5 = nn.Linear(embed_dim, h3_dim)\n",
    "        self.fc6 = nn.Linear(h3_dim, h2_dim)\n",
    "        self.fc7 = nn.Linear(h2_dim, h1_dim)\n",
    "        self.fc8 = nn.Linear(h1_dim, input_dim)\n",
    "        # define activation\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def qy_graph(self, x):\n",
    "        qy_logit = self.sigmoid(self.fc03(self.relu(self.fc02(self.relu(self.fc01(x))))))\n",
    "        qy = self.softmax(qy_logit)\n",
    "        return qy\n",
    "    \n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        eps = torch.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(eps)\n",
    "        return eps.mul(std).add_(mu)\n",
    "    \n",
    "    def qz_graph(self, x, y):\n",
    "        xy = torch.cat([x, y], 1)\n",
    "        h3 = self.relu(self.fc3(self.relu(self.fc2(self.relu(self.fc1(xy))))))\n",
    "        zm = self.fc41(h3)\n",
    "        logzv = self.fc42(h3)\n",
    "        z = self.reparametrize(zm, logzv)\n",
    "        return zm, logzv, z\n",
    "\n",
    "    def px_graph(self, y, z):\n",
    "        #--p(z|y)\n",
    "        prior_zm = self.fc13(y)\n",
    "        prior_logzv = self.fc14(y)\n",
    "        \n",
    "        #-- p(x|z)\n",
    "        recon = self.sigmoid(self.fc8(self.relu(self.fc7(self.relu(self.fc6(self.relu(self.fc5(z))))))))\n",
    "        return prior_zm, prior_logzv, recon\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        for i in range(y_dim):\n",
    "            one_hot_y = np.zeros((x.size(0), y_dim)) + np.eye(y_dim)[i]\n",
    "            prior_y = Variable(torch.from_numpy(one_hot_y).float())\n",
    "            local_output = {}\n",
    "            qy = self.qy_graph(x)\n",
    "            zm, logzv, z = self.qz_graph(x, prior_y)\n",
    "            prior_zm, prior_logzv, recon = self.px_graph(prior_y, z)\n",
    "            outputs.append((qy, prior_y, zm, logzv, z, prior_zm, prior_logzv, recon))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define ELOB loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def entropy(qy):\n",
    "    log_qy = torch.log(qy)\n",
    "    return -torch.sum(log_qy * qy, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def yRegularizationLoss(qy):\n",
    "    return -entropy(qy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def log_normal(z, zm, logzv):\n",
    "    zv = logzv.exp_()\n",
    "    var_sum = (logzv + (z - zm) * (z - zm) / zv).add_(np.log(2 * np.pi))\n",
    "    return torch.sum(var_sum, 1).mul_(-0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def zRegularizationLoss(z, zm, logzv, prior_zm, prior_logzv):\n",
    "    return torch.sum(log_normal(z, zm, logzv) - log_normal(z, prior_zm, prior_logzv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BCELoss(output, target):\n",
    "    ele_product = target * torch.log(output) + \\\n",
    "     (Variable(torch.ones(target.size())) - target) * torch.log(Variable(torch.ones(output.size())) - output)\n",
    "    return -torch.sum(ele_product, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def labeled_loss(recon_x, x, z, zm, logzv, prior_zm, prior_logzv):\n",
    "    return BCELoss(recon_x, x) + \\\n",
    "        zRegularizationLoss(z, zm, logzv, prior_zm, prior_logzv) - np.log(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_function(x, forward_outputs):\n",
    "    final_loss =  Variable(torch.zeros(x.size(0)))\n",
    "    losses = [None] * y_dim\n",
    "    for i in range(y_dim):\n",
    "        qy, prior_y, zm, logzv, z, prior_zm, prior_logzv, recon_x = forward_outputs[i]\n",
    "        losses[i] = qy[:,i] * labeled_loss(recon_x, x, z, zm, logzv, prior_zm, prior_logzv)\n",
    "        final_loss.add_(losses[i])\n",
    "    final_loss.add_(yRegularizationLoss(qy))\n",
    "    \n",
    "    return torch.sum(final_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating condtional entropy while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "def avg_conditional_entropy(true_labels, predicted_labels):\n",
    "    pred_label_set = set(predicted_labels)\n",
    "    avg_con_entropy = 0\n",
    "    for label in pred_label_set:\n",
    "        label_indices = np.where(np.array(predicted_labels) == label)\n",
    "        local_true_labels = [true_labels[e] for e in label_indices[0]]\n",
    "        local_truelabel_counts = Counter(local_true_labels)\n",
    "        size_cluster = len(local_true_labels)\n",
    "        local_truelabel_hist = [(true_label, count/size_cluster) \\\n",
    "                                for (true_label, count) in local_truelabel_counts.items()]\n",
    "        local_con_entropy = 0\n",
    "        for (_, hist_value) in local_truelabel_hist:\n",
    "            local_con_entropy += - math.log(hist_value) * hist_value\n",
    "\n",
    "        avg_con_entropy += local_con_entropy\n",
    "    return avg_con_entropy/len(pred_label_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = GmVAE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tst_x_var = autograd.Variable(torch.Tensor(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "def adjust_learning_rate(optimizer, iteration):\n",
    "    lr = 0.0001 * (0.1 ** (iteration  // 10))\n",
    "    lr = max(lr, 1e-5)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, batch_data in enumerate(dataloader):\n",
    "        iteration = int(epoch * len(labels)/len(batch_data)) + batch_idx\n",
    "        adjust_learning_rate(optimizer, iteration)\n",
    "        batch_data = Variable(batch_data.float())\n",
    "        optimizer.zero_grad()\n",
    "        forward_outputs = model(batch_data)\n",
    "        loss = loss_function(batch_data, forward_outputs)\n",
    "        loss.backward()\n",
    "        train_loss += loss.data[0]\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} iteration: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, iteration, batch_idx * len(batch_data), len(dataloader.dataset),\n",
    "                100. * batch_idx / len(dataloader),\n",
    "                loss.data[0] / len(batch_data)))\n",
    "            \n",
    "            tst_y = model.qy_graph(tst_x_var)\n",
    "            tst_y_ndy = tst_y.data.numpy()\n",
    "            predicted_labels = np.argmax(tst_y_ndy, axis=1).tolist()\n",
    "            avg_con_entropy = avg_conditional_entropy(true_labels, predicted_labels)\n",
    "            print('====> Epoch: {} iteration: {}, Average Conditinoal Entropy: {:.4f}'.format(epoch, iteration, avg_con_entropy))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(dataloader.dataset)))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'trained_model.mdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trained_model = GmVAE()\n",
    "# trained_model.load_state_dict(torch.load('trained_model.mdl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finnally !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 iteration: 0 [0/60000 (0%)]\tLoss: 677.675313\n",
      "====> Epoch: 0 iteration: 0, Average Conditinoal Entropy: 1.9102\n",
      "Train Epoch: 0 iteration: 10 [2000/60000 (3%)]\tLoss: 591.363008\n",
      "====> Epoch: 0 iteration: 10, Average Conditinoal Entropy: 1.4452\n",
      "Train Epoch: 0 iteration: 20 [4000/60000 (7%)]\tLoss: 576.628320\n",
      "====> Epoch: 0 iteration: 20, Average Conditinoal Entropy: 1.1506\n",
      "Train Epoch: 0 iteration: 30 [6000/60000 (10%)]\tLoss: 564.834570\n",
      "====> Epoch: 0 iteration: 30, Average Conditinoal Entropy: 1.1506\n",
      "Train Epoch: 0 iteration: 40 [8000/60000 (13%)]\tLoss: 548.189453\n",
      "====> Epoch: 0 iteration: 40, Average Conditinoal Entropy: 1.1506\n",
      "Train Epoch: 0 iteration: 50 [10000/60000 (17%)]\tLoss: 537.398203\n",
      "====> Epoch: 0 iteration: 50, Average Conditinoal Entropy: 1.1506\n",
      "Train Epoch: 0 iteration: 60 [12000/60000 (20%)]\tLoss: 519.037031\n",
      "====> Epoch: 0 iteration: 60, Average Conditinoal Entropy: 1.3725\n",
      "Train Epoch: 0 iteration: 70 [14000/60000 (23%)]\tLoss: 496.104961\n",
      "====> Epoch: 0 iteration: 70, Average Conditinoal Entropy: 1.4719\n",
      "Train Epoch: 0 iteration: 80 [16000/60000 (27%)]\tLoss: 472.821641\n",
      "====> Epoch: 0 iteration: 80, Average Conditinoal Entropy: 1.4500\n",
      "Train Epoch: 0 iteration: 90 [18000/60000 (30%)]\tLoss: 443.576055\n",
      "====> Epoch: 0 iteration: 90, Average Conditinoal Entropy: 1.5676\n",
      "Train Epoch: 0 iteration: 100 [20000/60000 (33%)]\tLoss: 402.884844\n",
      "====> Epoch: 0 iteration: 100, Average Conditinoal Entropy: 2.1343\n",
      "Train Epoch: 0 iteration: 110 [22000/60000 (37%)]\tLoss: 341.734414\n",
      "====> Epoch: 0 iteration: 110, Average Conditinoal Entropy: 2.2217\n",
      "Train Epoch: 0 iteration: 120 [24000/60000 (40%)]\tLoss: 305.325352\n",
      "====> Epoch: 0 iteration: 120, Average Conditinoal Entropy: 1.8058\n",
      "Train Epoch: 0 iteration: 130 [26000/60000 (43%)]\tLoss: 179.187559\n",
      "====> Epoch: 0 iteration: 130, Average Conditinoal Entropy: 2.3012\n",
      "Train Epoch: 0 iteration: 140 [28000/60000 (47%)]\tLoss: 13.027438\n",
      "====> Epoch: 0 iteration: 140, Average Conditinoal Entropy: 2.3012\n",
      "Train Epoch: 0 iteration: 150 [30000/60000 (50%)]\tLoss: -380.723125\n",
      "====> Epoch: 0 iteration: 150, Average Conditinoal Entropy: 2.3012\n",
      "Train Epoch: 0 iteration: 160 [32000/60000 (53%)]\tLoss: -1508.720781\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-132-8aeafa34d2cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'GmVAE-{}-epoech.ph'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-129-cf0ad94f60ea>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     18\u001b[0m                 loss.data[0] / len(batch_data)))\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mtst_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqy_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtst_x_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mtst_y_ndy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtst_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mpredicted_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtst_y_ndy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-85-4ef6241d60ad>\u001b[0m in \u001b[0;36mqy_graph\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mqy_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mqy_logit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc03\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc02\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc01\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mqy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqy_logit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mqy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/hxiong/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/hxiong/anaconda3/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/hxiong/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m    536\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/hxiong/anaconda3/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36maddmm\u001b[0;34m(cls, *args)\u001b[0m\n\u001b[1;32m    895\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 897\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAddmm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/hxiong/anaconda3/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36m_blas\u001b[0;34m(cls, args, inplace)\u001b[0m\n\u001b[1;32m    891\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m                 \u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/hxiong/anaconda3/lib/python3.6/site-packages/torch/autograd/_functions/blas.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, add_matrix, matrix1, matrix2, alpha, beta, inplace)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         return torch.addmm(alpha, add_matrix, beta,\n\u001b[0;32m---> 26\u001b[0;31m                            matrix1, matrix2, out=output)\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(0, epochs + 1):\n",
    "    train(epoch)\n",
    "    if epoch % 10 == 0:\n",
    "        torch.save(model.state_dict(), 'GmVAE-{}-epoech.ph'.format(epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check out the trained model: generate a realistic example from 10-dim Guassian points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eps = torch.FloatTensor(np.zeros(10)).normal_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "one_example = model.decode(Variable(eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "one_example = one_example.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "one_example = np.reshape(one_example, (28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "one_example.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.imshow(one_example)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
