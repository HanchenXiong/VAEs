{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "train_mnist = loadmat('mnist_train.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = train_mnist['train_X']\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 1)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = train_mnist['train_labels']\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### initialize pytorch dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MyMNISTDataset(object):\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "    \n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "dataset = MyMNISTDataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=200, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### label-added VAE and multi-classifier model and training configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 10 \n",
    "rnd_seed = 5\n",
    "log_interval = 10\n",
    "\n",
    "\n",
    "input_dim, y_dim = 784, 100\n",
    "encode_h1_dim = input_dim + y_dim\n",
    "h1_dim, h2_dim, h3_dim, embed_dim, output_dim  = 500, 500, 2000, 10, 784\n",
    "qy_h_dim = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define GmVAE model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GmVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GmVAE, self).__init__()\n",
    "        # encoder phase\n",
    "        self.fc01 = nn.Linear(input_dim, qy_h_dim)    \n",
    "        self.fc02 = nn.Linear(qy_h_dim, y_dim)\n",
    "        self.fc1 = nn.Linear(encode_h1_dim, h1_dim)\n",
    "        self.fc2 = nn.Linear(h1_dim, h2_dim)\n",
    "        self.fc3 = nn.Linear(h2_dim, h3_dim)\n",
    "        self.fc41 = nn.Linear(h3_dim, embed_dim)\n",
    "        self.fc42 = nn.Linear(h3_dim, embed_dim)\n",
    "        # decoder phase\n",
    "        self.fc03 = nn.Linear(y_dim, embed_dim)\n",
    "        self.fc04 = nn.Linear(y_dim, embed_dim)\n",
    "        self.fc5 = nn.Linear(embed_dim, h3_dim)\n",
    "        self.fc6 = nn.Linear(h3_dim, h2_dim)\n",
    "        self.fc7 = nn.Linear(h2_dim, h1_dim)\n",
    "        self.fc8 = nn.Linear(h1_dim, input_dim)\n",
    "        # define activation\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def qy_graph(self, x):\n",
    "        qy_logit = self.sigmoid(self.fc02(self.relu(self.fc01(x))))\n",
    "        qy = self.softmax(qy_logit)\n",
    "        return qy_logit, qy\n",
    "    \n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        eps = torch.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(eps)\n",
    "        return eps.mul(std).add_(mu)\n",
    "    \n",
    "    def qz_graph(self, x, y):\n",
    "        xy = torch.cat([x, y], 1)\n",
    "        h3 = self.relu(self.fc3(self.relu(self.fc2(self.relu(self.fc1(xy))))))\n",
    "        zm = self.fc41(h3)\n",
    "        logzv = self.fc42(h3)\n",
    "        z = self.reparametrize(zm, logzv)\n",
    "        return zm, logzv, z\n",
    "\n",
    "    def px_graph(self, y, z):\n",
    "        #--p(z|y)\n",
    "        prior_zm = self.fc03(y)\n",
    "        prior_logzv = self.fc04(y)\n",
    "        \n",
    "        #-- p(x|z)\n",
    "        recon = self.sigmoid(self.fc8(self.relu(self.fc7(self.relu(self.fc6(self.relu(self.fc5(z))))))))\n",
    "        return prior_zm, prior_logzv, recon\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        for i in range(y_dim):\n",
    "            one_hot_y = np.zeros((x.size(0), y_dim)) + np.eye(y_dim)[i]\n",
    "            y_ = Variable(torch.from_numpy(one_hot_y).float())\n",
    "            local_output = {}\n",
    "            qy_logit, qy = self.qy_graph(x)\n",
    "            zm, zv, z = self.qz_graph(x, y_)\n",
    "            prior_zm, prior_zv, recon = self.px_graph(y_, z)\n",
    "            outputs.append((qy_logit, qy, zm, zv, z, prior_zm, prior_zv, recon))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define ELOB loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy_with_logit(qy_logit, qy):\n",
    "    mm = torch.nn.LogSoftmax()\n",
    "    log_q = mm(qy_logit)\n",
    "    return -torch.sum(qy_logit * qy, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def yRegularizationLoss(qy_logit, qy):\n",
    "    cross_entropy = cross_entropy_with_logit(qy_logit, qy)\n",
    "    return torch.sum(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_normal(z, zm, logzv):\n",
    "    zv = logzv.exp_()\n",
    "    var_sum = (torch.log(zv) + (z - zm) * (z - zm) / zv).add_(np.log(2 * np.pi))\n",
    "    return torch.sum(var_sum, 1).mul_(-0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def zRegularizationLoss(z, zm, logzv, prior_zm, prior_logzv):\n",
    "    return torch.sum(log_normal(z, zm, logzv) - log_normal(z, prior_zm, prior_logzv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reconstruction_function = nn.BCELoss()\n",
    "reconstruction_function.size_average = False\n",
    "\n",
    "def loss_function(x, forward_outputs):\n",
    "    final_loss = Variable(torch.zeros(1, ))\n",
    "\n",
    "    for i in range(y_dim):\n",
    "        qy_logit, qy, zm, zv, z, prior_zm, prior_zv, recon_x = forward_outputs[i]\n",
    "        loss = reconstruction_function(recon_x, x) + yRegularizationLoss(qy_logit, qy) \\\n",
    "                + zRegularizationLoss(z, zm , zv, prior_zm, prior_zv)\n",
    "        final_loss.add_(loss)\n",
    "        \n",
    "    return final_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = GmVAE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "dtype = torch.FloatTensor\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(dataloader):\n",
    "        data = Variable(data.float())\n",
    "        optimizer.zero_grad()\n",
    "        forward_outputs = model(data)\n",
    "        loss = loss_function(data, forward_outputs)\n",
    "        loss.backward()\n",
    "        train_loss += loss.data[0]\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(dataloader.dataset),\n",
    "                100. * batch_idx / len(dataloader),\n",
    "                loss.data[0] / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(dataloader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finnally !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 53375.140000\n",
      "Train Epoch: 1 [2000/60000 (3%)]\tLoss: 24086.872500\n",
      "Train Epoch: 1 [4000/60000 (7%)]\tLoss: 21827.015000\n",
      "Train Epoch: 1 [6000/60000 (10%)]\tLoss: 25476.042500\n",
      "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 10572.577500\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 10603.741250\n",
      "Train Epoch: 1 [12000/60000 (20%)]\tLoss: 9253.816250\n",
      "Train Epoch: 1 [14000/60000 (23%)]\tLoss: 25797.820000\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 21104.645000\n",
      "Train Epoch: 1 [18000/60000 (30%)]\tLoss: 20322.681250\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 21512.292500\n",
      "Train Epoch: 1 [22000/60000 (37%)]\tLoss: 18732.982500\n",
      "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 18449.575000\n",
      "Train Epoch: 1 [26000/60000 (43%)]\tLoss: 21950.220000\n",
      "Train Epoch: 1 [28000/60000 (47%)]\tLoss: 17094.113750\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check out the trained model: generate a realistic example from 10-dim Guassian points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eps = torch.FloatTensor(np.zeros(10)).normal_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "one_example = model.decode(Variable(eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "one_example = one_example.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "one_example = np.reshape(one_example, (28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "one_example.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.imshow(one_example)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
